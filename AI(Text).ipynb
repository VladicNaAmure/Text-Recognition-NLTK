{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI(Text)Exercise5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qleX4HsLg0j",
        "colab_type": "text"
      },
      "source": [
        "Ссылка, на источник текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gt3Yi-fLHrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DATA_URL = \"http://az.lib.ru/t/tolstoj_a_k/text_0180.shtml\" #test\n",
        "DATA_URL = \"http://az.lib.ru/l/leskow_n_s/text_0246.shtml\" #Andrew's"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pNhbSm3Ld10",
        "colab_type": "text"
      },
      "source": [
        "Устанавливаем библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3FHUtVnLTl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q PyYaml==5.3.1\n",
        "! pip install -q rnnmorph==0.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWfUq3tSLmR_",
        "colab_type": "text"
      },
      "source": [
        "Создаём объект морфологического анализатора RNNMorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T4DzIS5Lped",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\") #ru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf_sm3u7LsUB",
        "colab_type": "text"
      },
      "source": [
        "Скачиваем текст, по которому будет дано задание, с помощью urllib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1jey8l6LvVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "raw_text = resource.read().decode(resource.headers.get_content_charset()) #Текс с html тегами\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlD1EluaL0FO",
        "colab_type": "text"
      },
      "source": [
        "Удаляем из текста HTML-теги с помощью простого регулярного выражения (используем их, так как структура документа и небольшие ошибки с извлечением текста нас мало волнуют)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEw-GvMJL20u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "clean_pattern = re.compile(\"<.*?>\")\n",
        "\n",
        "def clean_html(raw_html: str):\n",
        "  clean_text = re.sub(clean_pattern, \" \", raw_html)\n",
        "  return clean_text\n",
        "\n",
        "cleaned_text = clean_html(raw_text)#Текс без html тегов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjCCYaOiL7n7",
        "colab_type": "text"
      },
      "source": [
        "С помощью библиотеки NLTK разбиваем текст на предложения и токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuhqdyMJL9v_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "305d466a-fb0b-46e3-abb2-edfe5c137f78"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]\n",
        "\"A total of %d 'sentences'\" % len(tokenized_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A total of 468 'sentences'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2maibCMP2Z",
        "colab_type": "text"
      },
      "source": [
        "Задание 1.\n",
        "С помощью метода str.isalpha из стандартной библиотеки Python модифицируйте нижеследующий код так, чтобы в predictions остались только буквенные токены."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91IZAJ1zMR7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6e203450-e4d0-4417-e6b3-dd0ee66fc452"
      },
      "source": [
        "from tqdm import tqdm\n",
        "predictions = [[pred.normal_form for pred in sent if pred.normal_form.isalpha()]\n",
        "               for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\")]\n",
        "#predictions[-12:-11] #Сейчас видно, что токены типа \"точка\", \"запятая\", \"дефис\" и тд пока присутствуют в предложениях. От них нужно избавиться"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "sentences: 100%|██████████| 468/468 [00:00<00:00, 101140.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcCNo5vXjY4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f165da1-6a74-43de-e987-02588260ec97"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "468"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtKCNozqjnkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3538bfee-b816-48fb-eb57-bba574714599"
      },
      "source": [
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
        "len(non_uniq_tokens) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo9jcIryr4zw",
        "colab_type": "text"
      },
      "source": [
        "## Задание 2\n",
        "\n",
        "Используя `non_uniq_tokens`, стоп-слова для русского языка из библиотеки nltk (`nltk.corpus.stopwords`) и `nltk.FreqDist`, вычислите, **какую долю среди 100 самых частотных** токенов в произведении занимают токены, **не относящиеся** к стоп словам. \n",
        "\n",
        "**Не бойтесь использовать документацию NLTK и тьюториалы.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFRjEZrmr6it",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4d6fdba3-13f5-41a3-923f-6d1207ff30a5"
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "stopwords.words(\"russian\")[:5] #Пример стоп слов"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и', 'в', 'во', 'не', 'что']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uoWJOncmbeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a27c201d-7e5c-4260-e95f-0801e6e7d75b"
      },
      "source": [
        "freq_words = FreqDist()\n",
        "for word in non_uniq_tokens:\n",
        "     freq_words[word.lower()]+=1\n",
        "\n",
        "sto_freq_words = freq_words.most_common(100)\n",
        "\n",
        "filtered_sentence = [w for w in freq_words if freq_words[w.lower()] > 8 and not w in stop_words]\n",
        "sto_freq_words, len(filtered_sentence)/100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('nbsp', 876),\n",
              "  ('и', 495),\n",
              "  ('он', 363),\n",
              "  ('а', 261),\n",
              "  ('в', 254),\n",
              "  ('не', 238),\n",
              "  ('что', 169),\n",
              "  ('на', 133),\n",
              "  ('говорить', 122),\n",
              "  ('весь', 100),\n",
              "  ('быть', 93),\n",
              "  ('государь', 89),\n",
              "  ('левша', 87),\n",
              "  ('это', 85),\n",
              "  ('я', 85),\n",
              "  ('как', 83),\n",
              "  ('с', 81),\n",
              "  ('так', 74),\n",
              "  ('у', 72),\n",
              "  ('мы', 68),\n",
              "  ('плат', 65),\n",
              "  ('но', 52),\n",
              "  ('же', 52),\n",
              "  ('к', 49),\n",
              "  ('один', 48),\n",
              "  ('англичанин', 48),\n",
              "  ('такой', 48),\n",
              "  ('вы', 48),\n",
              "  ('свой', 47),\n",
              "  ('этот', 44),\n",
              "  ('потому', 42),\n",
              "  ('только', 42),\n",
              "  ('отвечать', 42),\n",
              "  ('тот', 41),\n",
              "  ('чтобы', 41),\n",
              "  ('мочь', 41),\n",
              "  ('за', 38),\n",
              "  ('который', 37),\n",
              "  ('из', 37),\n",
              "  ('то', 36),\n",
              "  ('сейчас', 36),\n",
              "  ('ничто', 35),\n",
              "  ('очень', 34),\n",
              "  ('по', 32),\n",
              "  ('мастер', 32),\n",
              "  ('от', 31),\n",
              "  ('самый', 31),\n",
              "  ('блоха', 30),\n",
              "  ('ещё', 30),\n",
              "  ('ты', 30),\n",
              "  ('наш', 30),\n",
              "  ('себя', 29),\n",
              "  ('да', 29),\n",
              "  ('другой', 28),\n",
              "  ('сказать', 27),\n",
              "  ('тут', 26),\n",
              "  ('до', 26),\n",
              "  ('знать', 25),\n",
              "  ('какой', 25),\n",
              "  ('стать', 24),\n",
              "  ('взять', 23),\n",
              "  ('потом', 23),\n",
              "  ('нет', 23),\n",
              "  ('дело', 23),\n",
              "  ('глава', 21),\n",
              "  ('русский', 21),\n",
              "  ('смотреть', 21),\n",
              "  ('теперь', 21),\n",
              "  ('человек', 20),\n",
              "  ('если', 20),\n",
              "  ('видеть', 20),\n",
              "  ('мой', 19),\n",
              "  ('рука', 19),\n",
              "  ('бы', 18),\n",
              "  ('время', 18),\n",
              "  ('тогда', 18),\n",
              "  ('ни', 18),\n",
              "  ('ваш', 17),\n",
              "  ('уже', 17),\n",
              "  ('даже', 17),\n",
              "  ('сделать', 17),\n",
              "  ('спрашивать', 17),\n",
              "  ('сам', 17),\n",
              "  ('для', 16),\n",
              "  ('хотеть', 16),\n",
              "  ('аглицкий', 16),\n",
              "  ('работа', 16),\n",
              "  ('где', 15),\n",
              "  ('слово', 15),\n",
              "  ('большой', 15),\n",
              "  ('велеть', 15),\n",
              "  ('платов', 15),\n",
              "  ('вот', 15),\n",
              "  ('мелкоскоп', 15),\n",
              "  ('николай', 14),\n",
              "  ('там', 14),\n",
              "  ('хороший', 14),\n",
              "  ('под', 14),\n",
              "  ('делать', 14),\n",
              "  ('нимфозорий', 14)],\n",
              " 0.79)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJJnJ3vxTCrK",
        "colab_type": "text"
      },
      "source": [
        "Задание 3.\n",
        "Вычислите, сколько токенов встречается в тексте строго больше 50 раз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0vbRp3Fyo_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ceb6849-8823-4f5e-9c1d-3146c30c9334"
      },
      "source": [
        "tokens_upper_freq = [w for w in freq_words if freq_words[w.lower()] > 10]\n",
        "len(tokens_upper_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}